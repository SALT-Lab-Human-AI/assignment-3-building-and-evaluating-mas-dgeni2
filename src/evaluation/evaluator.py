"""
System Evaluator
Runs batch evaluations and generates reports.

Example usage:
    # Load config
    with open("config.yaml") as f:
        config = yaml.safe_load(f)
    
    # Initialize evaluator with orchestrator
    evaluator = SystemEvaluator(config, orchestrator=my_orchestrator)
    
    # Run evaluation
    report = await evaluator.evaluate_system("data/test_queries.json")
    
    # Results are automatically saved to outputs/
"""

from typing import Dict, Any, List, Optional
import json
import logging
from pathlib import Path
from datetime import datetime
import asyncio

from .judge import LLMJudge


class SystemEvaluator:
    """
    Evaluates the multi-agent system using test queries and LLM-as-a-Judge.

    TODO: YOUR CODE HERE
    - Load test queries from file
    - Run system on all test queries
    - Collect and aggregate results
    - Generate evaluation report
    - Perform error analysis
    """

    def __init__(self, config: Dict[str, Any], orchestrator=None):
        """
        Initialize evaluator.

        Args:
            config: Configuration dictionary (from config.yaml)
            orchestrator: The orchestrator to evaluate
        """
        self.config = config
        self.orchestrator = orchestrator
        self.logger = logging.getLogger("evaluation.evaluator")

        # Load evaluation configuration from config.yaml
        eval_config = config.get("evaluation", {})
        self.enabled = eval_config.get("enabled", True)
        self.max_test_queries = eval_config.get("num_test_queries", None)
        
        # Initialize judge (passes config to load judge model settings and criteria)
        self.judge = LLMJudge(config)

        # Evaluation results
        self.results: List[Dict[str, Any]] = []
        
        self.logger.info(f"SystemEvaluator initialized (enabled={self.enabled})")

    async def evaluate_system(
        self,
        test_queries_path: str = "data/test_queries.json"
    ) -> Dict[str, Any]:
        """
        Run full system evaluation.

        Args:
            test_queries_path: Path to test queries JSON file

        Returns:
            Evaluation results and statistics

        TODO: YOUR CODE HERE
        - Load test queries
        - Run system on each query
        - Evaluate each response
        - Aggregate results
        - Generate report
        """
        # Check if evaluation is enabled in config.yaml
        if not self.enabled:
            self.logger.warning("Evaluation is disabled in config.yaml")
            return {"error": "Evaluation is disabled in configuration"}
        
        self.logger.info("Starting system evaluation")

        # Load test queries
        test_queries = self._load_test_queries(test_queries_path)
        self.logger.info(f"Loaded {len(test_queries)} test queries")

        # Evaluate each query
        for i, test_case in enumerate(test_queries, 1):
            query_text = test_case.get("query", "")[:60]
            print(f"\n[{i}/{len(test_queries)}] Processing: {query_text}...")
            self.logger.info(f"Evaluating query {i}/{len(test_queries)}: {query_text}")

            try:
                result = await self._evaluate_query(test_case)
                self.results.append(result)
                print(f"[{i}/{len(test_queries)}] ✓ Completed")
            except Exception as e:
                print(f"[{i}/{len(test_queries)}] ✗ Error: {str(e)[:100]}")
                self.logger.error(f"Error evaluating query {i}: {e}", exc_info=True)
                self.results.append({
                    "query": test_case.get("query", ""),
                    "error": str(e)
                })

        # Aggregate results
        report = self._generate_report()

        # Save results
        self._save_results(report)

        return report

    async def _evaluate_query(self, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """
        Evaluate a single test query.

        Args:
            test_case: Test case with query and optional ground truth

        Returns:
            Evaluation result for this query

        This shows how to integrate with the orchestrator.
        """
        query = test_case.get("query", "")
        ground_truth = test_case.get("ground_truth")
        expected_sources = test_case.get("expected_sources", [])

        # Run through orchestrator if available
        if self.orchestrator:
            try:
                print(f"  → Running through orchestrator...")
                # Call orchestrator's process_query method (synchronous)
                # Run in executor to avoid blocking the async event loop
                import concurrent.futures
                loop = asyncio.get_event_loop()
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    response_data = await loop.run_in_executor(
                        executor, 
                        self.orchestrator.process_query, 
                        query
                    )
                print(f"  → Orchestrator completed")
                
                # Extract response and metadata
                if isinstance(response_data, dict):
                    # Ensure we have the expected structure
                    if "response" not in response_data:
                        response_data["response"] = response_data.get("output", "")
                    
                    # Extract sources from metadata
                    metadata = response_data.get("metadata", {})
                    sources = metadata.get("sources", [])
                    citations = metadata.get("citations", [])
                    
                    # If sources not in metadata, try to extract from response
                    if not sources and citations:
                        sources = [{"title": cit, "type": "citation"} for cit in citations[:10]]
                    
                    response_data["metadata"]["sources"] = sources
                    response_data["citations"] = citations
                
            except Exception as e:
                self.logger.error(f"Error processing query through orchestrator: {e}", exc_info=True)
                response_data = {
                    "query": query,
                    "response": f"Error: {str(e)}",
                    "citations": [],
                    "metadata": {"error": str(e), "sources": []}
                }
        else:
            # Placeholder for testing without orchestrator
            self.logger.warning("No orchestrator provided, using placeholder response")
            response_data = {
                "query": query,
                "response": "Placeholder response - orchestrator not connected",
                "citations": [],
                "metadata": {"num_sources": 0}
            }

        # Evaluate response using LLM-as-a-Judge
        print(f"  → Evaluating with LLM-as-a-Judge...")
        evaluation = await self.judge.evaluate(
            query=query,
            response=response_data.get("response", ""),
            sources=response_data.get("metadata", {}).get("sources", []),
            ground_truth=ground_truth
        )
        print(f"  → Judge score: {evaluation.get('overall_score', 0.0):.3f}")

        return {
            "query": query,
            "response": response_data.get("response", ""),
            "evaluation": evaluation,
            "metadata": response_data.get("metadata", {}),
            "ground_truth": ground_truth
        }

    def _load_test_queries(self, path: str) -> List[Dict[str, Any]]:
        """
        Load test queries from JSON file.

        TODO: YOUR CODE HERE
        - Create test query dataset
        - Load and validate queries
        """
        path_obj = Path(path)
        if not path_obj.exists():
            self.logger.warning(f"Test queries file not found: {path}")
            return []

        with open(path_obj, 'r') as f:
            queries = json.load(f)

        # Limit number of queries if configured in config.yaml
        if self.max_test_queries and len(queries) > self.max_test_queries:
            self.logger.info(f"Limiting to {self.max_test_queries} queries (from config.yaml)")
            queries = queries[:self.max_test_queries]

        return queries

    def _generate_report(self) -> Dict[str, Any]:
        """
        Generate evaluation report with statistics and analysis.

        TODO: YOUR CODE HERE
        - Calculate aggregate statistics
        - Identify best/worst performing queries
        - Analyze errors
        - Generate visualizations (optional)
        """
        if not self.results:
            return {"error": "No results to report"}

        # Calculate statistics
        total_queries = len(self.results)
        successful = [r for r in self.results if "error" not in r]
        failed = [r for r in self.results if "error" in r]

        # Aggregate scores
        criterion_scores = {}
        overall_scores = []

        for result in successful:
            evaluation = result.get("evaluation", {})
            overall_scores.append(evaluation.get("overall_score", 0.0))

            # Collect scores by criterion
            for criterion, score_data in evaluation.get("criterion_scores", {}).items():
                if criterion not in criterion_scores:
                    criterion_scores[criterion] = []
                criterion_scores[criterion].append(score_data.get("score", 0.0))

        # Calculate averages
        avg_overall = sum(overall_scores) / len(overall_scores) if overall_scores else 0.0

        avg_criterion_scores = {}
        for criterion, scores in criterion_scores.items():
            avg_criterion_scores[criterion] = sum(scores) / len(scores) if scores else 0.0

        # Find best and worst
        best_result = max(successful, key=lambda r: r.get("evaluation", {}).get("overall_score", 0.0)) if successful else None
        worst_result = min(successful, key=lambda r: r.get("evaluation", {}).get("overall_score", 0.0)) if successful else None
        
        # Calculate score distribution
        score_ranges = {
            "excellent (0.9-1.0)": sum(1 for s in overall_scores if s >= 0.9),
            "good (0.7-0.9)": sum(1 for s in overall_scores if 0.7 <= s < 0.9),
            "average (0.5-0.7)": sum(1 for s in overall_scores if 0.5 <= s < 0.7),
            "below_average (0.3-0.5)": sum(1 for s in overall_scores if 0.3 <= s < 0.5),
            "poor (0.0-0.3)": sum(1 for s in overall_scores if s < 0.3)
        }
        
        # Calculate standard deviation
        import statistics
        score_std = statistics.stdev(overall_scores) if len(overall_scores) > 1 else 0.0
        
        # Analyze errors
        error_analysis = []
        for result in failed:
            error_analysis.append({
                "query": result.get("query", ""),
                "error": result.get("error", "Unknown error")
            })
        
        # Criterion analysis (which criteria are strongest/weakest)
        criterion_analysis = {}
        for criterion, scores in criterion_scores.items():
            criterion_analysis[criterion] = {
                "average": sum(scores) / len(scores) if scores else 0.0,
                "min": min(scores) if scores else 0.0,
                "max": max(scores) if scores else 0.0,
                "std": statistics.stdev(scores) if len(scores) > 1 else 0.0
            }

        report = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_queries": total_queries,
                "successful": len(successful),
                "failed": len(failed),
                "success_rate": len(successful) / total_queries if total_queries > 0 else 0.0
            },
            "scores": {
                "overall_average": avg_overall,
                "overall_std": score_std,
                "overall_min": min(overall_scores) if overall_scores else 0.0,
                "overall_max": max(overall_scores) if overall_scores else 0.0,
                "by_criterion": avg_criterion_scores,
                "criterion_analysis": criterion_analysis,
                "distribution": score_ranges
            },
            "best_result": {
                "query": best_result.get("query", "") if best_result else "",
                "score": best_result.get("evaluation", {}).get("overall_score", 0.0) if best_result else 0.0,
                "criterion_scores": best_result.get("evaluation", {}).get("criterion_scores", {}) if best_result else {}
            } if best_result else None,
            "worst_result": {
                "query": worst_result.get("query", "") if worst_result else "",
                "score": worst_result.get("evaluation", {}).get("overall_score", 0.0) if worst_result else 0.0,
                "criterion_scores": worst_result.get("evaluation", {}).get("criterion_scores", {}) if worst_result else {}
            } if worst_result else None,
            "error_analysis": error_analysis,
            "detailed_results": self.results
        }

        return report

    def _save_results(self, report: Dict[str, Any]):
        """
        Save evaluation results to file.

        TODO: YOUR CODE HERE
        - Save detailed results
        - Generate visualizations
        - Create summary report
        """
        output_dir = Path("outputs")
        output_dir.mkdir(exist_ok=True)

        # Save detailed results
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = output_dir / f"evaluation_{timestamp}.json"

        with open(results_file, 'w') as f:
            json.dump(report, f, indent=2)

        self.logger.info(f"Evaluation results saved to {results_file}")

        # Save summary
        summary_file = output_dir / f"evaluation_summary_{timestamp}.txt"
        with open(summary_file, 'w') as f:
            f.write("EVALUATION SUMMARY\n")
            f.write("=" * 70 + "\n\n")
            f.write(f"Evaluation Date: {report.get('timestamp', 'Unknown')}\n\n")

            summary = report.get("summary", {})
            f.write("SUMMARY STATISTICS\n")
            f.write("-" * 70 + "\n")
            f.write(f"Total Queries: {summary.get('total_queries', 0)}\n")
            f.write(f"Successful: {summary.get('successful', 0)}\n")
            f.write(f"Failed: {summary.get('failed', 0)}\n")
            f.write(f"Success Rate: {summary.get('success_rate', 0.0):.2%}\n\n")

            scores = report.get("scores", {})
            f.write("OVERALL SCORES\n")
            f.write("-" * 70 + "\n")
            f.write(f"Average Score: {scores.get('overall_average', 0.0):.3f}\n")
            f.write(f"Standard Deviation: {scores.get('overall_std', 0.0):.3f}\n")
            f.write(f"Min Score: {scores.get('overall_min', 0.0):.3f}\n")
            f.write(f"Max Score: {scores.get('overall_max', 0.0):.3f}\n\n")

            f.write("SCORES BY CRITERION\n")
            f.write("-" * 70 + "\n")
            for criterion, score in scores.get("by_criterion", {}).items():
                analysis = scores.get("criterion_analysis", {}).get(criterion, {})
                f.write(f"{criterion}:\n")
                f.write(f"  Average: {score:.3f}\n")
                f.write(f"  Range: {analysis.get('min', 0.0):.3f} - {analysis.get('max', 0.0):.3f}\n")
                f.write(f"  Std Dev: {analysis.get('std', 0.0):.3f}\n\n")

            f.write("SCORE DISTRIBUTION\n")
            f.write("-" * 70 + "\n")
            distribution = scores.get("distribution", {})
            for range_name, count in distribution.items():
                f.write(f"{range_name}: {count}\n")
            f.write("\n")

            # Best and worst results
            best = report.get("best_result")
            worst = report.get("worst_result")
            if best:
                f.write("BEST RESULT\n")
                f.write("-" * 70 + "\n")
                f.write(f"Query: {best.get('query', '')}\n")
                f.write(f"Score: {best.get('score', 0.0):.3f}\n\n")
            
            if worst:
                f.write("WORST RESULT\n")
                f.write("-" * 70 + "\n")
                f.write(f"Query: {worst.get('query', '')}\n")
                f.write(f"Score: {worst.get('score', 0.0):.3f}\n\n")
            
            # Error analysis
            errors = report.get("error_analysis", [])
            if errors:
                f.write("ERROR ANALYSIS\n")
                f.write("-" * 70 + "\n")
                for error in errors:
                    f.write(f"Query: {error.get('query', '')}\n")
                    f.write(f"Error: {error.get('error', '')}\n\n")

        self.logger.info(f"Summary saved to {summary_file}")

    def export_for_report(self, output_path: str = "outputs/report_data.json"):
        """
        Export data formatted for inclusion in technical report.

        """
        if not self.results:
            self.logger.warning("No results to export")
            return
        
        # Create output directory
        output_dir = Path(output_path).parent
        output_dir.mkdir(exist_ok=True)
        
        # Format data for report
        report_data = {
            "evaluation_date": datetime.now().isoformat(),
            "total_queries": len(self.results),
            "results": self.results
        }
        
        with open(output_path, 'w') as f:
            json.dump(report_data, f, indent=2)
        
        self.logger.info(f"Report data exported to {output_path}")


async def example_simple_evaluation():
    """
    Example 1: Simple evaluation without orchestrator
    Tests the evaluation pipeline with mock responses
    
    Usage:
        import asyncio
        from src.evaluation.evaluator import example_simple_evaluation
        asyncio.run(example_simple_evaluation())
    """
    import yaml
    from dotenv import load_dotenv
    
    load_dotenv()
    
    print("=" * 70)
    print("EXAMPLE 1: Simple Evaluation (No Orchestrator)")
    print("=" * 70)
    
    # Load config
    with open("config.yaml", 'r') as f:
        config = yaml.safe_load(f)
    
    # Create test queries in memory (no file needed)
    test_queries = [
        {
            "query": "What is the capital of France?",
            "ground_truth": "Paris is the capital of France."
        },
        {
            "query": "What are the benefits of exercise?",
            "ground_truth": "Exercise improves physical health, mental wellbeing, and reduces disease risk."
        }
    ]
    
    # Save test queries temporarily
    test_file = Path("data/test_queries_example.json")
    test_file.parent.mkdir(exist_ok=True)
    with open(test_file, 'w') as f:
        json.dump(test_queries, f, indent=2)
    
    # Initialize evaluator without orchestrator
    evaluator = SystemEvaluator(config, orchestrator=None)
    
    print("\nRunning evaluation on test queries...")
    print("Note: Using placeholder responses since no orchestrator is connected\n")
    
    # Run evaluation
    report = await evaluator.evaluate_system(str(test_file))
    
    # Display results
    print("\n" + "=" * 70)
    print("EVALUATION RESULTS")
    print("=" * 70)
    print(f"\nTotal Queries: {report['summary']['total_queries']}")
    print(f"Successful: {report['summary']['successful']}")
    print(f"Failed: {report['summary']['failed']}")
    print(f"Overall Average Score: {report['scores']['overall_average']:.3f}\n")
    
    print("Scores by Criterion:")
    for criterion, score in report['scores']['by_criterion'].items():
        print(f"  {criterion}: {score:.3f}")
    
    print(f"\nDetailed results saved to outputs/")
    
    # Clean up
    test_file.unlink()


    async def example_with_orchestrator():
        """
    Example 2: Evaluation with orchestrator
    Shows how to connect the evaluator to your multi-agent system
    
    Usage:
        import asyncio
        from src.evaluation.evaluator import example_with_orchestrator
        asyncio.run(example_with_orchestrator())
    """
    import yaml
    from dotenv import load_dotenv
    
    load_dotenv()
    
    print("=" * 70)
    print("EXAMPLE 2: Evaluation with Orchestrator")
    print("=" * 70)
    
    # Load config
    with open("config.yaml", 'r') as f:
        config = yaml.safe_load(f)
    
    # Initialize orchestrator (try sequential first, then AutoGen)
    orchestrator = None
    try:
        from src.orchestrator import Orchestrator
        orchestrator = Orchestrator(config)
        print("\nSequential orchestrator initialized successfully")
    except Exception as e:
        print(f"\nCould not initialize sequential orchestrator: {e}")
        try:
            from src.autogen_orchestrator import AutoGenOrchestrator
            orchestrator = AutoGenOrchestrator(config)
            print("AutoGen orchestrator initialized successfully")
        except Exception as e2:
            print(f"Could not initialize AutoGen orchestrator: {e2}")
            print("This example requires a working orchestrator implementation")
            return
    
    # Create test queries
    test_queries = [
        {
            "query": "What are the key principles of accessible user interface design?",
            "ground_truth": "Key principles include perceivability, operability, understandability, and robustness."
        }
    ]
    
    test_file = Path("data/test_queries_orchestrator.json")
    test_file.parent.mkdir(exist_ok=True)
    with open(test_file, 'w') as f:
        json.dump(test_queries, f, indent=2)
    
    # Initialize evaluator with orchestrator
    evaluator = SystemEvaluator(config, orchestrator=orchestrator)
    
    print("\nRunning evaluation with real orchestrator...")
    print("This will actually query your multi-agent system\n")
    
    # Run evaluation
    report = await evaluator.evaluate_system(str(test_file))
    
    # Display results
    print("\n" + "=" * 70)
    print("EVALUATION RESULTS")
    print("=" * 70)
    print(f"\nTotal Queries: {report['summary']['total_queries']}")
    print(f"Overall Average Score: {report['scores']['overall_average']:.3f}\n")
    
    print("Scores by Criterion:")
    for criterion, score in report['scores']['by_criterion'].items():
        print(f"  {criterion}: {score:.3f}")
    
    # Show detailed result for first query
    if report['detailed_results']:
        result = report['detailed_results'][0]
        print("\n" + "=" * 70)
        print("DETAILED RESULT (First Query)")
        print("=" * 70)
        print(f"\nQuery: {result['query']}")
        print(f"\nResponse: {result['response'][:200]}...")
        print(f"\nOverall Score: {result['evaluation']['overall_score']:.3f}")
    
    print(f"\nFull results saved to outputs/")
    
    # Clean up
    test_file.unlink()


# For direct execution
if __name__ == "__main__":
    import asyncio
    
    print("Running SystemEvaluator Examples\n")
    
    # Run example 1
    asyncio.run(example_simple_evaluation())
    
    print("\n\n")
    
    # Run example 2 (if orchestrator is available)
    asyncio.run(example_with_orchestrator())
